<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>aychao</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-02-11T08:26:11.293Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/12/30/JAVA%E5%B8%B8%E8%A7%81%E5%BC%82%E5%B8%B8/"/>
    <id>http://example.com/2022/12/30/JAVA%E5%B8%B8%E8%A7%81%E5%BC%82%E5%B8%B8/</id>
    <published>2022-12-30T02:58:54.907Z</published>
    <updated>2023-02-11T08:26:11.293Z</updated>
    
    <content type="html"><![CDATA[<h1 id="JAVA常见异常"><a href="#JAVA常见异常" class="headerlink" title="JAVA常见异常"></a>JAVA常见异常</h1><h2 id="Java-io-NullPointerException"><a href="#Java-io-NullPointerException" class="headerlink" title="Java.io.NullPointerException"></a>Java.io.NullPointerException</h2><ul><li>null 空的，不存在的</li><li>NullPointer 空指针</li></ul><p>空指针异常，该异常出现在我们操作某个对象的属性或方法时，如果该对象是null时引发。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">str.length();<span class="comment">//空指针异常</span></span><br></pre></td></tr></table></figure><p>上述代码中引用类型变量str的值为null，此时不能通过它调用字符串的方法或引用属性，否则就会引发空指针异常。</p><p>解决办法:</p><p>找到为什么赋值为null，确保该对象的值不能为null再操作属性或方法即可。</p><h2 id="java-lang-NumberFormatException-For-input-string-“xxxxx”"><a href="#java-lang-NumberFormatException-For-input-string-“xxxxx”" class="headerlink" title="java.lang.NumberFormatException: For input string: “xxxxx”"></a>java.lang.NumberFormatException: For input string: “xxxxx”</h2><ul><li>Number 数字</li><li>Format 格式</li></ul><p>数字格式异常，该异常通常出现在我们使用包装类将一个字符串解析为对应的基本类型时引发。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> <span class="string">&quot;123.123&quot;</span>;<span class="comment">//小数不能转换为整数!</span></span><br><span class="line"><span class="type">int</span> <span class="variable">d</span> <span class="operator">=</span> Integer.parseInt(line);<span class="comment">//抛出异常NumberFormatException</span></span><br><span class="line">System.out.println(d);</span><br></pre></td></tr></table></figure><p>上述代码中由于line的字符串内容是”123.123”.而这个数字是不能通过包装类Integer解析为一个整数因此出现该异常。注:非数字的字符出在解析时也会出现该异常。</p><p>解决办法:</p><p>确保解析的字符串正确表达了基本类型可以保存的值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> <span class="string">&quot;123&quot;</span>;</span><br><span class="line"><span class="type">int</span> <span class="variable">d</span> <span class="operator">=</span> Integer.parseInt(line);</span><br><span class="line">System.out.println(d);<span class="comment">//123</span></span><br></pre></td></tr></table></figure><h2 id="java-lang-StringIndexOutOfBoundsException"><a href="#java-lang-StringIndexOutOfBoundsException" class="headerlink" title="java.lang.StringIndexOutOfBoundsException"></a>java.lang.StringIndexOutOfBoundsException</h2><ul><li>index 索引，下标</li><li>Bounds 边界</li><li>OutOfBounds 超出了边界</li></ul><p>字符串下标越界异常。该异常通常出现在String对应的方法中，当我们指定的下标小于0或者大于等于字符串的长度时会抛出该异常。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;thinking in java&quot;</span>;</span><br><span class="line"><span class="type">char</span> <span class="variable">c</span> <span class="operator">=</span> str.charAt(<span class="number">20</span>);<span class="comment">//出现异常</span></span><br><span class="line">System.out.println(c);</span><br></pre></td></tr></table></figure><p>解决办法:</p><p>指定下标时的范围应当在&gt;&#x3D;0并且&lt;&#x3D;字符串的长度。</p><h2 id="java-io-InvalidClassException"><a href="#java-io-InvalidClassException" class="headerlink" title="java.io.InvalidClassException"></a>java.io.InvalidClassException</h2><ul><li>Invalid 无效的</li><li>Class 类</li></ul><p>无效的类异常，该异常出现在使用java.io.ObjectInputStream在进行对象反序列化时在readObject()方法中抛出。这通常是因为反序列化的对象版本号与该对象所属类现有的版本号不一致导致的。</p><p>可以通过在类上使用常量:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br></pre></td></tr></table></figure><p>来固定版本号，这样序列化的对象就可以进行反序列化了。</p><p>JAVA建议我们实现Serializable接口的类主动定义序列化版本号,若不定义编译器会在编译时<br>根据当前类结构生成版本号,但弊端是只要这个类内容发生了改变,那么再次编译时版本号就会改变,直接的后果就是之前序列化的对象都无法再进行反序列化.</p><p>如果自行定义版本号,那么可以在改变类内容的同时不改变版本号,这样一来,反序列化以前的<br>对象时对象输入流会采取兼容模式,即:当前类的属性在反序列化的对象中还存在的则直接还原,不存在的就是用该属性的默认值</p><p>出现该异常的解决办法:</p><ol><li>首先使用上述常量固定版本号</li><li>重新序列化对象(将对象通过ObjectOutputStream重新序列化并写出)</li><li>再进行反序列化即可</li></ol><p>需要注意，之前没有定义序列化版本号时序列化后的对象都无法再反序列化回来，所以若写入了文件，可将之前的那些文件都删除，避免读取即可。</p><h2 id="java-io-NotSerializableException"><a href="#java-io-NotSerializableException" class="headerlink" title="java.io.NotSerializableException"></a>java.io.NotSerializableException</h2><ul><li>NotSerializable 不能序列化</li></ul><p>不能序列化异常，该异常通常出现在我们使用java.io.ObjectOutputStream进行对象序列化(调用writeObject)时。原因时序列化的对象所属的类没有实现java.io.Serializable接口导致</p><p>出现该异常的解决办法:</p><p>将序列化的类实现该接口即可</p><h2 id="java-io-UnsupportedEncodingException"><a href="#java-io-UnsupportedEncodingException" class="headerlink" title="java.io.UnsupportedEncodingException"></a>java.io.UnsupportedEncodingException</h2><ul><li>Unsupported 不支持的</li><li>Encoding字符集</li></ul><p>不支持的字符集异常，该异常通常出现在使用字符串形式指定字符集名字时，犹豫字符集名字拼写错误导致。例如</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PrintWriter</span> <span class="variable">pw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PrintWriter</span>(<span class="string">&quot;pw.txt&quot;</span>, <span class="string">&quot;UFT-8&quot;</span>);</span><br></pre></td></tr></table></figure><p>上述代码中，字符集拼写成”UFT-8”就是拼写错误。</p><p>常见的字符集名字:</p><ul><li>GBK:我国的国标编码，其中英文1个字节，中文2字节</li><li>UTF-8:unicode的传输编码，也称为万国码。其中英文1字节，中文3字节。</li><li>ISO8859-1:欧中的字符集，不支持中文。</li></ul><h2 id="java-io-FileNotFoundException"><a href="#java-io-FileNotFoundException" class="headerlink" title="java.io.FileNotFoundException"></a>java.io.FileNotFoundException</h2><ul><li>File 文件</li><li>NotFound 没有找到</li></ul><p>文件没有找到异常，该异常通常出现在我们使用文件输入流读取指定路径对应的文件时出现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">FileInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="string">&quot;f1os.dat&quot;</span>);</span><br></pre></td></tr></table></figure><p>上述代码如果指定的文件f1os.dat文件不在当前目录下，就会引发该异常：</p><p>java.io.FileNotFoundException: f1os.dat (系统找不到指定的文件。)</p><p>注:</p><p>抽象路径”f1os.dat”等同于”.&#x2F;f1os.dat”。因此该路径表示当前目录下应当有一个名为f1os.dat的文件。</p><p>还经常出现在文件输出流写出文件时，指定的路径无法将该文件创建出来时出现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="string">&quot;./a/fos.dat&quot;</span>);</span><br></pre></td></tr></table></figure><p>上述代码中，如果当前目录下没有a目录，那么就无法在该目录下自动创建文件fos.dat，此时也会引发这个异常。</p><p>其他API上出现该异常通常也是上述类似的原因导致的。</p><p>解决办法:</p><p>在读取文件时，确保指定的路径正确，且文件名拼写正确。</p><p>在写出文件时，确保指定的文件所在的目录存在。</p><h2 id="java-net-ConnectException-Connection-refused-connect"><a href="#java-net-ConnectException-Connection-refused-connect" class="headerlink" title="java.net.ConnectException: Connection refused: connect"></a>java.net.ConnectException: Connection refused: connect</h2><ul><li>connection 连接</li><li>refused 拒绝</li></ul><p>连接异常,连接被拒绝了.这通常是客户端在使用Socket与远端计算机建立连接时由于指定的地址或端口无效导致无法连接服务端引起的.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">&quot;正在连接服务端...&quot;</span>);</span><br><span class="line"><span class="type">Socket</span> <span class="variable">socket</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Socket</span>(<span class="string">&quot;localhost&quot;</span>,<span class="number">8088</span>);<span class="comment">//这里可能引发异常</span></span><br><span class="line">System.out.println(<span class="string">&quot;与服务端建立连接!&quot;</span>);</span><br></pre></td></tr></table></figure><p>解决办法:</p><ul><li>检查客户端实例化Socket时指定的地址和端口是否正常</li><li>客户端连接前,服务端是否已经启动了</li></ul><h2 id="java-net-BindException-Address-already-in-use"><a href="#java-net-BindException-Address-already-in-use" class="headerlink" title="java.net.BindException: Address already in use"></a>java.net.BindException: Address already in use</h2><ul><li>bind 绑定</li><li>address 地址</li><li>already 已经</li><li>Address already in use 地址已经被使用了</li></ul><p>绑定异常,该异常通常是在创建ServerSocket时指定的服务端口已经被系统其他程序占用导致的.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">&quot;正在启动服务端...&quot;</span>);</span><br><span class="line"><span class="type">ServerSocket</span> <span class="variable">serverSocket</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ServerSocket</span>(<span class="number">8088</span>);<span class="comment">//这里可能引发异常</span></span><br><span class="line">System.out.println(<span class="string">&quot;服务端启动完毕&quot;</span>);</span><br></pre></td></tr></table></figure><p>解决办法:</p><ul><li>有可能是重复启动了服务端导致的,先将之前启动的服务端关闭</li><li>找到该端口被占用的程序,将其进程结束</li><li>重新指定一个新的服务端口在重新启动服务端</li></ul><h2 id="java-net-SocketException-Connection-reset"><a href="#java-net-SocketException-Connection-reset" class="headerlink" title="java.net.SocketException: Connection reset"></a>java.net.SocketException: Connection reset</h2><ul><li>socket 套接字</li><li>net 网络</li><li>reset 重置</li></ul><p>套接字异常,链接重置。这个异常通常出现在Socket进行的TCP链接时，由于远端计算机异常断开(在没有调用socket.close()的之前直接结束了程序)导致的。</p><p>解决办法:</p><ul><li>无论是客户端还是服务端当希望与另一端断开连接时，应当调用socket.close()方法，此时会进行TCP的挥手断开动作。</li><li>这个异常是无法完全避免的，因为无法保证程序在没有调用socket.close()前不被强制杀死。</li></ul><h2 id="java-lang-InterruptedException"><a href="#java-lang-InterruptedException" class="headerlink" title="java.lang.InterruptedException"></a>java.lang.InterruptedException</h2><ul><li>interrupt 中断</li></ul><p>中断异常.这个异常通常在一个线程调用了会产生阻塞的方法处于阻塞的过程中,此时该线程的interrupt()方法被调用.那么阻塞方法会立即抛出中断异常并停止线程的阻塞使其继续运行.</p><p>例如:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果线程t1调用Thread.sleep(1000)处于阻塞的过程中,其他线程调用了t1线程的inerrupt()方法,那么t1调用的sleep()方法就会立即抛出中断异常InterruptedException并停止阻塞.</p><h2 id="java-util-NoSuchElementException"><a href="#java-util-NoSuchElementException" class="headerlink" title="java.util.NoSuchElementException"></a>java.util.NoSuchElementException</h2><ul><li>such这个</li><li>Element元素</li></ul><p>没有这个元素的异常.该异常通常发生在使用迭代器Iterator遍历集合元素时由于没有先通过hasNext()方法判断存在下一个元素而贸然通过next()获取下一个元素时产生(当集合所有元素都经过迭代器遍历一遍后还使用next获取).</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> (String)it.next();</span><br><span class="line"><span class="comment">//这里就可能产生NoSuchException异常</span></span><br><span class="line">            System.out.println(it.next());</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>上述代码中循环遍历时,每次调用hasNext()确定存在下一个元素时,循环里面连续调用过两次next()方法,这意味着第二次调用next()方法时并没有判断是否还存在.所以在最后会出现异常.</p><p>解决办法:</p><p>保证每次调用next()方法前都确定hasNext()为true才进行即可.</p><h2 id="java-util-ConcurrentModificationException"><a href="#java-util-ConcurrentModificationException" class="headerlink" title="java.util.ConcurrentModificationException"></a>java.util.ConcurrentModificationException</h2><p>Concurrent 并发</p><p>Modification 修改</p><p>并发修改异常.这个异常也经常出现在使用迭代器遍历集合时产生.</p><p>当我们使用一个迭代器遍历集合的过程中,通过集合的方法增删元素时,迭代器会抛出该异常.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line"><span class="comment">//出现ConcurrentModificationException</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> (String)it.next();</span><br><span class="line">    <span class="keyword">if</span>(<span class="string">&quot;#&quot;</span>.equals(str))&#123;</span><br><span class="line">        c.remove(str);<span class="comment">//遍历过程中不要通过集合方法增或删元素</span></span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(str);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解决办法:</p><p>使用迭代器提供的remove()方法可以删除通过next()获取的元素.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> (String)it.next();</span><br><span class="line">            <span class="keyword">if</span>(<span class="string">&quot;#&quot;</span>.equals(str))&#123;</span><br><span class="line"><span class="comment">//                c.remove(str);</span></span><br><span class="line">                it.remove();</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(str);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h2 id="java-lang-UnsupportedOperationException"><a href="#java-lang-UnsupportedOperationException" class="headerlink" title="java.lang.UnsupportedOperationException"></a>java.lang.UnsupportedOperationException</h2><p>support 支持</p><p>unsupported 不支持的</p><p>operation 操作</p><p>不支持的操作异常.该异常出现在很多的API中.</p><p>例如:常出现在我们对数组转换的集合进行增删元素操作时抛出.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">String[] array = &#123;<span class="string">&quot;one&quot;</span>,<span class="string">&quot;two&quot;</span>,<span class="string">&quot;three&quot;</span>,<span class="string">&quot;four&quot;</span>,<span class="string">&quot;five&quot;</span>&#125;;</span><br><span class="line">System.out.println(<span class="string">&quot;array:&quot;</span>+ Arrays.toString(array));</span><br><span class="line">List&lt;String&gt; list = Arrays.asList(array);<span class="comment">//将数组转换为一个List集合</span></span><br><span class="line">System.out.println(<span class="string">&quot;list:&quot;</span>+list);</span><br><span class="line"></span><br><span class="line">list.set(<span class="number">0</span>,<span class="string">&quot;six&quot;</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;list:&quot;</span>+list);</span><br><span class="line"><span class="comment">//对该集合的操作就是对原数组的操作</span></span><br><span class="line">System.out.println(<span class="string">&quot;array:&quot;</span>+ Arrays.toString(array));</span><br><span class="line"></span><br><span class="line"><span class="comment">//由于数组是定长的,因此任何会改变数组长度的操作都是不支持的!</span></span><br><span class="line">list.add(<span class="string">&quot;seven&quot;</span>);<span class="comment">//UnsupportedOperationException</span></span><br></pre></td></tr></table></figure><h2 id="java-lang-IllegalArgumentException-wrong-number-of-arguments"><a href="#java-lang-IllegalArgumentException-wrong-number-of-arguments" class="headerlink" title="java.lang.IllegalArgumentException: wrong number of arguments"></a>java.lang.IllegalArgumentException: wrong number of arguments</h2><p>Illegal非法的</p><p>Argument参数</p><p>wrong number of arguments 参数的数量有误</p><p>非法的参数异常.该异常出现在反射API中</p><p>例如:当我们用Method的invode方法反射调用一个有参方法，而指定的实参个数不符合时出现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Person</span> <span class="variable">p</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Person</span>();</span><br><span class="line">p.say(<span class="string">&quot;你好!&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//实例化</span></span><br><span class="line"><span class="type">Class</span> <span class="variable">cls</span> <span class="operator">=</span> Class.forName(<span class="string">&quot;reflect.Person&quot;</span>);</span><br><span class="line"><span class="type">Object</span> <span class="variable">o</span> <span class="operator">=</span> cls.newInstance();</span><br><span class="line"><span class="comment">//调用方法</span></span><br><span class="line"><span class="comment">//获取say(String,int)方法</span></span><br><span class="line"><span class="type">Method</span> <span class="variable">m</span> <span class="operator">=</span> cls.getMethod(<span class="string">&quot;say&quot;</span>,String.class,<span class="type">int</span>.class);</span><br><span class="line"><span class="comment">//m.invoke(o,&quot;嘿嘿&quot;);//反射调用时值传递了一个String参数，此时会抛出异常</span></span><br><span class="line"><span class="comment">//正确写法</span></span><br><span class="line">m.invoke(o,<span class="string">&quot;嘿嘿&quot;</span>,<span class="number">1</span>);<span class="comment">//需要传递第二个参数，该参数为一个int值。</span></span><br></pre></td></tr></table></figure><h2 id="java-lang-ClassNotFoundException-xxxx-包名-XXXXX-类名"><a href="#java-lang-ClassNotFoundException-xxxx-包名-XXXXX-类名" class="headerlink" title="java.lang.ClassNotFoundException: xxxx(包名).XXXXX(类名)"></a>java.lang.ClassNotFoundException: xxxx(包名).XXXXX(类名)</h2><p>ClassNotFound 类没有找到</p><p>类没有找到异常.该异常出现在反射API中</p><p>例如:当我们使用一个类的完全限定名使用Class.forName()加载这个类的类对象时，如果指定的完全限定名拼写有误会出现该异常</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//加载reflect包中的类Person</span></span><br><span class="line"><span class="comment">//Class cls = Class.forName(&quot;reflect.Person1&quot;);//完全限定名拼写错误会时出现异常</span></span><br><span class="line"><span class="comment">//正确写法</span></span><br><span class="line"><span class="type">Class</span> <span class="variable">cls</span> <span class="operator">=</span> Class.forName(<span class="string">&quot;reflect.Person&quot;</span>);</span><br></pre></td></tr></table></figure><h2 id="java-lang-NoSuchMethodException"><a href="#java-lang-NoSuchMethodException" class="headerlink" title="java.lang.NoSuchMethodException"></a>java.lang.NoSuchMethodException</h2><p>NoSuchMethod 没有这个方法</p><p>没有这个方法异常，该异常通常出现在反射API中</p><p>例如:当我们通过类对象Class获取其表示的类中某个指定的方法时，如果指定的方法名错误或参数列表错误时会导致该异常的抛出:</p><p>Person类定义:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> reflect;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Person</span> &#123;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">say</span><span class="params">(String info)</span>&#123;</span><br><span class="line">        System.out.println(name+<span class="string">&quot;:&quot;</span>+info);</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">watchTV</span><span class="params">()</span>&#123;</span><br><span class="line">        System.out.println(name+<span class="string">&quot;看电视&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>反射操作代码片段:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//实例化</span></span><br><span class="line"><span class="type">Class</span> <span class="variable">cls</span> <span class="operator">=</span> Class.forName(<span class="string">&quot;reflect.Person&quot;</span>);</span><br><span class="line"><span class="comment">//调用方法</span></span><br><span class="line"><span class="comment">//Method m = cls.getMethod(&quot;says&quot;,String.class);//没有says(String)这个方法，导致报错</span></span><br><span class="line"><span class="comment">//Method m = cls.getMethod(&quot;say&quot;,String.class,int.class);//参数列表错误，导致报错</span></span><br><span class="line"><span class="type">Method</span> <span class="variable">m</span> <span class="operator">=</span> cls.getMethod(<span class="string">&quot;say&quot;</span>,String.class);<span class="comment">//正确</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;JAVA常见异常&quot;&gt;&lt;a href=&quot;#JAVA常见异常&quot; class=&quot;headerlink&quot; title=&quot;JAVA常见异常&quot;&gt;&lt;/a&gt;JAVA常见异常&lt;/h1&gt;&lt;h2 id=&quot;Java-io-NullPointerException&quot;&gt;&lt;a href=&quot;#J</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark HA &amp; Yarn配置</title>
    <link href="http://example.com/2022/05/22/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/22/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-22T03:06:37.000Z</published>
    <updated>2022-05-24T09:35:11.714Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、Spark-Standalone-HA模式"><a href="#一、Spark-Standalone-HA模式" class="headerlink" title="一、Spark-Standalone-HA模式"></a>一、Spark-Standalone-HA模式</h3><p>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。</p><p>1.此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper<br>2.配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>3.在master节点上重新进行前面配置的 zookeeper 操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件 </span><br><span class="line">在 /export/server 目录下创建软连接 </span><br><span class="line">进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg </span><br><span class="line">接上步给 zoo.cfg 添加内容 </span><br><span class="line">进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去</span><br><span class="line">将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 </span><br><span class="line">推送成功后，分别在 slave1 和 slave2 上创建软连接 </span><br><span class="line">接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹 下的 myid 中的内容分别改为 2 和 3 </span><br><span class="line">配置环境变量： </span><br><span class="line">因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处.</span><br></pre></td></tr></table></figure><p>4.进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/conf </span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><p>5.为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">结果显示： </span><br><span class="line">...... </span><br><span class="line">82 <span class="comment"># 告知Spark的master运行在哪个机器上 </span></span><br><span class="line">83 <span class="comment"># export SPARK_MASTER_HOST=master </span></span><br><span class="line">.........</span><br></pre></td></tr></table></figure><p> 文末添加内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -  # spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="string">Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 -  # 指定Zookeeper的连接地址</span></span><br><span class="line"><span class="string">Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span>  <span class="comment"># 指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></figure><p>6.分发 spark-env.sh 到 salve1 和 slave2 上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh slave1:/export/server/spark/conf/ </span><br><span class="line"></span><br><span class="line">scp spark-env.sh slave2:/export/server/spark/conf/</span><br></pre></td></tr></table></figure><p>7.启动之前确保 Zookeeper 和 HDFS 均已经启动<br>  启动集群:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 master 上 启动一个master 和全部worker </span></span><br><span class="line">/export/server/spark/sbin/start-all.sh </span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master </span></span><br><span class="line">/export/server/spark/sbin/start-master.sh</span><br><span class="line"></span><br><span class="line">jps  <span class="comment">#查看是否启动</span></span><br></pre></td></tr></table></figure><p>8.访问 WebUI 界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://master:8081/</span><br><span class="line"></span><br><span class="line">http://slave1:8082/</span><br></pre></td></tr></table></figure><p>9.此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># master主机 master 的进程号 </span></span><br><span class="line"><span class="built_in">kill</span> -9 41589</span><br></pre></td></tr></table></figure><p>10.访问 slave1 的 WebUI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://slave1:8082/</span><br></pre></td></tr></table></figure><p>11.进行主备切换的测试  提交一个 spark 任务到当前 活跃的 master上 :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/export/server/spark/bin/spark-submit --master spark://master:7077 </span><br><span class="line"></span><br><span class="line">/export/server/spark/examples/src/main/python/pi.py 1000</span><br></pre></td></tr></table></figure><p>12.复制标签 kill 掉 master 的 进程号  再次访问 master 的 WebUI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://master:8081/</span><br><span class="line"></span><br><span class="line">网页访问不了！</span><br></pre></td></tr></table></figure><p>13.再次访问 slave1 的 WebUI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://slave1:8082/</span><br></pre></td></tr></table></figure><p>14.可以看到当前活跃的 master 提示信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/export/server/spark/bin/spark-submit --master</span><br><span class="line"></span><br><span class="line">同样可以输出结果</span><br></pre></td></tr></table></figure><p>  当新的 master 接收集群后, 程序继续运行, 正常得到结果.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">结论 HA模式下, 主备切换 不会影响到正在运行的程序.</span><br><span class="line">最大的影响是 会让它中断大约30秒左右</span><br></pre></td></tr></table></figure><h3 id="二、Spark-On-YARN模式"><a href="#二、Spark-On-YARN模式" class="headerlink" title="二、Spark On YARN模式"></a>二、Spark On YARN模式</h3><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端</p><p>1.保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-env.sh 文件部分显示：</span><br><span class="line">···</span><br><span class="line"> 77 <span class="comment">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 </span></span><br><span class="line"> 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line"> 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">····</span><br></pre></td></tr></table></figure><p>2.链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master yarn --deploy-mode client|cluster</span><br><span class="line"><span class="comment"># --deploy-mode 选项是指定部署模式, 默认是 客户端模式 </span></span><br><span class="line"><span class="comment"># client就是客户端模式 </span></span><br><span class="line"><span class="comment"># cluster就是集群模式 </span></span><br><span class="line"><span class="comment"># --deploy-mode 仅可以用在YARN模式下</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master yarn --deploy-mode client|cluster</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数</span><br></pre></td></tr></table></figure><p>3.park-submit 和 spark-shell 和 pyspark的相关参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- bin/pyspark: pyspark解释器spark环境 - bin/spark-shell: scala解释器spark环境 </span><br><span class="line">- bin/spark-submit: 提交jar包或Python文件执行的工具 </span><br><span class="line">- bin/spark-sql: sparksql客户端工具</span><br><span class="line"></span><br><span class="line">这4个客户端工具的参数基本通用.以spark-submit 为例: </span><br><span class="line">bin/spark-submit --master spark://master:7077 xxx.py</span><br></pre></td></tr></table></figure><p>4.启动 YARN 的历史服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/hadoop-3.3.0/sbin </span><br><span class="line"></span><br><span class="line">./mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>5.访问WebUI界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://master:19888/</span><br></pre></td></tr></table></figure><p> client 模式测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/export/server/spark <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --master yarn --deploy-mode client --</span><br><span class="line">driver-memory 512m --executor-memory 512m --num-executors 1 --total- </span><br><span class="line">executor-cores 2 <span class="variable">$&#123;SPARK_HOME&#125;</span>/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure><p> cluster 模式测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/export/server/spark <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --master yarn --deploy-mode cluster --driver- </span><br><span class="line">memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores </span><br><span class="line">2 --conf <span class="string">&quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot;</span> --conf </span><br><span class="line"><span class="string">&quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot;</span> </span><br><span class="line"><span class="variable">$&#123;SPARK_HOME&#125;</span>/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;一、Spark-Standalone-HA模式&quot;&gt;&lt;a href=&quot;#一、Spark-Standalone-HA模式&quot; class=&quot;headerlink&quot; title=&quot;一、Spark-Standalone-HA模式&quot;&gt;&lt;/a&gt;一、Spark-Standalon</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark-local&amp; stand-alone配置</title>
    <link href="http://example.com/2022/05/22/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/2022/05/22/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-22T03:06:37.000Z</published>
    <updated>2022-05-23T09:48:11.092Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、Spark-local模式"><a href="#一、Spark-local模式" class="headerlink" title="一、Spark-local模式"></a>一、Spark-local模式</h3><p>本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境</p><p>  Anaconda On Linux 安装 (单台服务器脚本安装)</p><p>  1.安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行文件</span></span><br><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">过程显示：</span><br><span class="line">···</span><br><span class="line"><span class="comment">#出现内容选 yes</span></span><br><span class="line">Please answer <span class="string">&#x27;yes&#x27;</span> or <span class="string">&#x27;no&#x27;</span>:<span class="string">&#x27; </span></span><br><span class="line"><span class="string">&gt;&gt;&gt; yes</span></span><br><span class="line"><span class="string">···</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 出现添加路径：/export/server/anaconda3</span></span><br><span class="line"><span class="string">···</span></span><br><span class="line"><span class="string">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3 </span></span><br><span class="line"><span class="string">PREFIX=/export/server/anaconda3</span></span><br><span class="line"><span class="string">···</span></span><br></pre></td></tr></table></figure><p>2.安装完成后, 退出终端， 重新进来:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果显示： </span><br><span class="line"><span class="comment">#看到这个Base开头表明安装好了.base是默认的虚拟环境. </span></span><br><span class="line">Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 </span><br><span class="line">(base) [root@master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>3.在虚拟环境内安装包 （有WARNING不用管）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>4.spark 安装<br>  将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 </span></span><br><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure><p>  建立软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><p>  添加环境变量<br>  SPARK_HOME: 表示Spark安装路径在哪里<br>  PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器<br>  JAVA_HOME: 告知Spark Java在哪里<br>  HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里<br>  HADOOP_HOME: 告知Spark Hadoop安装在哪里</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">内容：</span><br><span class="line">···</span><br><span class="line">注：此部分之前配置过，此部分不需要在配置</span><br><span class="line"><span class="comment">#JAVA_HOME </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241 </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin </span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop-3.3.0 </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="comment">#ZOOKEEPER_HOME</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/export/server/zookeeper </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br><span class="line">·····</span><br><span class="line"><span class="comment">#将以下部分添加进去</span></span><br><span class="line"><span class="comment">#SPARK_HOME </span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/server/spark</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP_CONF_DIR export </span></span><br><span class="line">HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment">#PYSPARK_PYTHON </span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line"></span><br><span class="line">内容添加进去：</span><br><span class="line"><span class="comment">#JAVA_HOME </span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line"><span class="comment">#PYSPARK_PYTHON </span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure><p>5.重新加载环境变量文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile </span><br><span class="line"><span class="built_in">source</span> ~/.bashrc\</span><br></pre></td></tr></table></figure><p> 进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/anaconda3/envs/pyspark/bin/</span><br></pre></td></tr></table></figure><p> 开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./pyspark</span><br></pre></td></tr></table></figure><p>6.查看WebUI界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#浏览器访问：</span></span><br><span class="line">http://master:4040/</span><br></pre></td></tr></table></figure><p> 退出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h3 id="二、Spark-Standalone模式"><a href="#二、Spark-Standalone模式" class="headerlink" title="二、Spark-Standalone模式"></a>二、Spark-Standalone模式</h3><p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境<br>Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)<br>1.安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行文件 </span></span><br><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">过程显示： </span><br><span class="line">... </span><br><span class="line"><span class="comment"># 出现内容选 yes </span></span><br><span class="line">Please answer <span class="string">&#x27;yes&#x27;</span> or <span class="string">&#x27;no&#x27;</span>:<span class="string">&#x27; </span></span><br><span class="line"><span class="string">&gt;&gt;&gt; yes</span></span><br><span class="line"><span class="string">···</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 出现添加路径：/export/server/anaconda3</span></span><br><span class="line"><span class="string">···</span></span><br><span class="line"><span class="string">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3 </span></span><br><span class="line"><span class="string">PREFIX=/export/server/anaconda3</span></span><br><span class="line"><span class="string">···</span></span><br></pre></td></tr></table></figure><p>2.安装完成后, 退出终端， 重新进来:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果显示： </span><br><span class="line"><span class="comment"># 看到这个Base开头表明安装好了.base是默认的虚拟环境. </span></span><br><span class="line">Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 </span><br><span class="line">(base) [root@master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>3.在 node1节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分发 .bashrc : </span></span><br><span class="line">scp ~/.bashrc root@node2:~/ </span><br><span class="line">scp ~/.bashrc root@node3:~/</span><br><span class="line"></span><br><span class="line"><span class="comment">#分发 profile : </span></span><br><span class="line">scp /etc/profile/ root@node2:/etc/ </span><br><span class="line">scp /etc/profile/ root@node3:/etc/</span><br></pre></td></tr></table></figure><p>4.创建虚拟环境 pyspark 基于 python3.8</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure><p>5.切换到虚拟环境内</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br><span class="line"></span><br><span class="line">结果显示： </span><br><span class="line">(base) [root@master ~]<span class="comment"># conda activate pyspark </span></span><br><span class="line">(pyspark) [root@master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>6.在虚拟环境内安装包 （有WARNING不用管）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>7.master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/conf</span><br></pre></td></tr></table></figure><p>8.将文件 workers.template 改名为 workers，并配置文件内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> workers.template workers</span><br><span class="line"></span><br><span class="line"><span class="comment"># localhost删除，内容追加文末：</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br><span class="line"><span class="comment"># 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</span></span><br></pre></td></tr></table></figure><p>9.将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"></span><br><span class="line">文末追加内容：</span><br><span class="line"></span><br><span class="line"><span class="comment">##设置JAVA安装目录 </span></span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"></span><br><span class="line"><span class="comment">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 </span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop </span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment">## 指定spark老大Master的IP和提交任务的通信端口 </span></span><br><span class="line"><span class="comment"># 告知Spark的master运行在哪个机器上 </span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=master </span><br><span class="line"><span class="comment"># 告知sparkmaster的通讯端口 </span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077 </span><br><span class="line"><span class="comment"># 告知spark master的 webui端口 </span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># worker cpu可用核数 </span></span><br><span class="line">SPARK_WORKER_CORES=1 </span><br><span class="line"><span class="comment"># worker可用内存 </span></span><br><span class="line">SPARK_WORKER_MEMORY=1g </span><br><span class="line"><span class="comment"># worker的工作通讯地址 </span></span><br><span class="line">SPARK_WORKER_PORT=7078 </span><br><span class="line"><span class="comment"># worker的 webui地址 </span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置历史服务器 </span></span><br><span class="line"><span class="comment"># 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 </span></span><br><span class="line">SPARK_HISTORY_OPTS=<span class="string">&quot;- </span></span><br><span class="line"><span class="string">Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - </span></span><br><span class="line"><span class="string">Dspark.history.fs.cleaner.enabled=true&quot;</span></span><br></pre></td></tr></table></figure><p>10.开启 hadoop 的 hdfs 和 yarn 集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh </span><br><span class="line"></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>11.在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mkdir</span> /sparklog </span><br><span class="line"></span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /sparklog</span><br></pre></td></tr></table></figure><p>12.将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf </span><br><span class="line"></span><br><span class="line">vim spark-defaults.conf</span><br><span class="line"></span><br><span class="line">文末追加内容为： </span><br><span class="line"><span class="comment"># 开启spark的日期记录功能 </span></span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span> </span><br><span class="line"><span class="comment"># 设置spark日志记录的路径 </span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span> hdfs://master:8020/sparklog/ </span><br><span class="line"><span class="comment"># 设置spark日志是否启动压缩 </span></span><br><span class="line">spark.eventLog.compress   <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>13.配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为<br>log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为<br>WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> log4j.properties.template log4j.properties </span><br><span class="line"></span><br><span class="line">vim log4j.properties</span><br><span class="line"></span><br><span class="line">结果显示： </span><br><span class="line">... </span><br><span class="line">18 <span class="comment"># Set everything to be logged to the console </span></span><br><span class="line">19 log4j.rootCategory=WARN, console </span><br><span class="line">....</span><br></pre></td></tr></table></figure><p>14.node1节点分发 spark安装文件夹到node2和node3上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"></span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:<span class="variable">$PWD</span> </span><br><span class="line"></span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><p>15.node 2和node3上做软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><p>16.重新加载环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line">进入 /export/server/spark/sbin 文件目录下 启动 start-history-server.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /export/server/spark/sbin </span><br><span class="line"></span><br><span class="line">./start-history-server.sh</span><br></pre></td></tr></table></figure><p> 访问WebUI 界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问： </span><br><span class="line"></span><br><span class="line">http://master:18080/</span><br></pre></td></tr></table></figure><p>18.启动Spark的Master和Worker进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动全部master和worker </span></span><br><span class="line">sbin/start-all.sh </span><br><span class="line"><span class="comment"># 或者可以一个个启动: </span></span><br><span class="line"><span class="comment"># 启动当前机器的master </span></span><br><span class="line">sbin/start-master.sh </span><br><span class="line"><span class="comment"># 启动当前机器的worker </span></span><br><span class="line">sbin/start-worker.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止全部 </span></span><br><span class="line">sbin/stop-all.sh </span><br><span class="line"><span class="comment"># 停止当前机器的master </span></span><br><span class="line">sbin/stop-master.sh </span><br><span class="line"><span class="comment"># 停止当前机器的worker </span></span><br><span class="line">sbin/stop-worker.sh</span><br></pre></td></tr></table></figure><p> 访问WebUI界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问： </span><br><span class="line"></span><br><span class="line">http://master:8080/</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;一、Spark-local模式&quot;&gt;&lt;a href=&quot;#一、Spark-local模式&quot; class=&quot;headerlink&quot; title=&quot;一、Spark-local模式&quot;&gt;&lt;/a&gt;一、Spark-local模式&lt;/h3&gt;&lt;p&gt;本地模式(单机) 本地模式就是以一个</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>spark基础环境配置</title>
    <link href="http://example.com/2022/05/22/%E2%80%9Cspark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E2%80%9D/"/>
    <id>http://example.com/2022/05/22/%E2%80%9Cspark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E2%80%9D/</id>
    <published>2022-05-22T03:06:37.000Z</published>
    <updated>2022-05-23T03:02:50.295Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、基础环境配置"><a href="#一、基础环境配置" class="headerlink" title="一、基础环境配置"></a>一、基础环境配置</h3><p>1.编辑主机名（3台机器）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hostname</span><br></pre></td></tr></table></figure><p>2.Hosts映射（3台主机）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p>3.关闭防火墙（3台机器）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld.service   <span class="comment">#关闭防火墙</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld.service <span class="comment">#禁止防火墙开启自启</span></span><br></pre></td></tr></table></figure><p>4.设置ssh免密登录（node1执行-&gt;node1|node2|node3）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen <span class="comment">#4个回车生成公钥、私钥</span></span><br><span class="line">ssh-copy-id node1、ssh-copy-id node2、ssh-copy-id node3</span><br></pre></td></tr></table></figure><p>5.集群时间同步（3台机器）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum-y install ntpdate</span><br><span class="line">ntpdate ntp4.aliyun.com</span><br></pre></td></tr></table></figure><p>6.创建统一工作目录（3台机器）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /export/server/    <span class="comment">#软件安装路径</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /export/data/      <span class="comment">#数据存储路径</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /export/software/  <span class="comment">#安装包存放路径</span></span><br></pre></td></tr></table></figure><h3 id="二、JDK安装"><a href="#二、JDK安装" class="headerlink" title="二、JDK安装"></a>二、JDK安装</h3><p>1.编译环境软件安装目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir-p /export/server/</span><br></pre></td></tr></table></figure><p>2.JDK1.8安装上传jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir-p /export/server/</span><br></pre></td></tr></table></figure><p>3.配置环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile </span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br></pre></td></tr></table></figure><p>4.重新加载环境变量文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>5.JDK配置后验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure><h3 id="三、Hadoop安装配置"><a href="#三、Hadoop安装配置" class="headerlink" title="三、Hadoop安装配置"></a>三、Hadoop安装配置</h3><p>1.解压上传文件（ hadoop-3.3.0-Centos7-64-with-snappy.tar.gz ）到&#x2F;export&#x2F;server&#x2F;目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</span><br></pre></td></tr></table></figure><p>2.修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改hadoop-env.sh</span></span><br><span class="line"><span class="built_in">cd</span> /export/server/hadoop-3.3.0/etc/hadoop</span><br><span class="line">vim hadoop-env.sh   <span class="comment">#进入文件</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241   <span class="comment">#文件最后添加</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改core-site.xml</span></span><br><span class="line">&lt;设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://node1:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;设置Hadoop本地保存数据路径&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;设置HDFS web UI用户身份&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;整合hive 用户代理设置&gt;</span><br><span class="line">hdfs-site.xml</span><br><span class="line">marpred-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;文件系统垃圾桶保存时间&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改mapred-site.xml</span></span><br><span class="line">&lt; 设置MR程序默认运行模式： yarn集群模式 <span class="built_in">local</span>本地模式&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;MR程序历史服务地址&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;node1:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;MR程序历史服务器web端地址&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">yarn-site.xml</span><br><span class="line">workers</span><br><span class="line">&lt;分发同步hadoop安装包&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改yarn-site.xml</span></span><br><span class="line">&lt; 设置YARN集群主角色运行机器位置 &gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;是否将对容器实施物理内存限制&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;是否将对容器实施虚拟内存限制&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;开启日志聚集&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;设置yarn历史服务器地址&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;历史日志保存的时间 7天&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改workers</span></span><br><span class="line">localhost</span><br><span class="line">node1.itcast.cn</span><br><span class="line">node2.itcast.cn</span><br><span class="line">node3.itcast.cn</span><br><span class="line"></span><br><span class="line"><span class="comment">#分发同步hadoop安装包</span></span><br><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line">scp -r hadoop-3.3.0 root@node2:<span class="variable">$PWD</span></span><br><span class="line">scp -r hadoop-3.3.0 root@node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><p>3.将hadoop添加到环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>4.重新加载环境变量文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>5.Hadoop集群启动 格式化namenode（只有首次启动需要格式化）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -forma</span><br></pre></td></tr></table></figure><p>6.脚本一键启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>7.查看WEB页面 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   HDFS集群 :http://node1:9870/</span><br><span class="line">YARN集群 :http://node1:8088/</span><br></pre></td></tr></table></figure><h3 id="四、Zookeeper安装配置"><a href="#四、Zookeeper安装配置" class="headerlink" title="四、Zookeeper安装配置"></a>四、Zookeeper安装配置</h3><p>1.配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 node1.root node2.root node3.root</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果显示</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.88.135 node1 node1.root</span><br><span class="line">192.168.88.136 node2 node2.root</span><br><span class="line">192.168.88.137 node3 node3.root</span><br></pre></td></tr></table></figure><p>2.zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz</span><br></pre></td></tr></table></figure><p>3.在 &#x2F;export&#x2F;server 目录下创建软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure><p>4.cd进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/conf/ </span><br><span class="line"><span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><p>接上步给 zoo.cfg 添加内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Zookeeper的数据存放目录</span></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"><span class="comment"># 保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># 日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="comment"># 集群中服务器地址</span></span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br><span class="line">server.3=node3:2888:3888</span><br></pre></td></tr></table></figure><p>5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/zkdata</span><br><span class="line"><span class="built_in">touch</span> myid</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1&#x27;</span> &gt; myid</span><br></pre></td></tr></table></figure><p>6.将 node1 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给node2 和 node3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/zookeeper-3.4.10/ node2:<span class="variable">$PWD</span></span><br><span class="line">scp -r /export/server/zookeeper-3.4.10/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><p>7.推送成功后，分别在 node2 和 node3 上创建软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure><p>接上步推送完成后将 node2 和 node3 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3</p><p>8.配置zookeeper的环境变量（注：三台主机都需要配置）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment">#zookeeper 环境变量</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>9.重新加载环境变量文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>10.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F; 目录下启动 zkServer.sh 脚本 （注：三台都需要做）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/bin/ </span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p>11.查看zookeeper是否开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;一、基础环境配置&quot;&gt;&lt;a href=&quot;#一、基础环境配置&quot; class=&quot;headerlink&quot; title=&quot;一、基础环境配置&quot;&gt;&lt;/a&gt;一、基础环境配置&lt;/h3&gt;&lt;p&gt;1.编辑主机名（3台机器）&lt;/p&gt;
&lt;figure class=&quot;highlight bas</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>我的第一篇博客文章</title>
    <link href="http://example.com/2022/05/21/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/"/>
    <id>http://example.com/2022/05/21/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/</id>
    <published>2022-05-21T06:57:06.000Z</published>
    <updated>2022-05-21T06:57:06.881Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/05/21/hello-world/"/>
    <id>http://example.com/2022/05/21/hello-world/</id>
    <published>2022-05-21T06:51:11.881Z</published>
    <updated>2022-05-21T06:51:11.882Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
