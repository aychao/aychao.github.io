{"meta":{"title":"aychao","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"Categories","date":"2022-05-21T07:21:50.403Z","updated":"2022-05-21T07:21:50.403Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2022-05-21T07:21:50.399Z","updated":"2022-05-21T07:21:50.399Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-21T07:21:50.412Z","updated":"2022-05-21T07:21:50.412Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"kafka API使用方法","slug":"kafka-API使用方法","date":"2022-06-12T09:47:51.000Z","updated":"2022-06-20T22:00:17.672Z","comments":true,"path":"2022/06/12/kafka-API使用方法/","link":"","permalink":"http://example.com/2022/06/12/kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","excerpt":"","text":"Produce流程图 1.一个正常的生产逻辑需要具备以下几个步骤(1)配置生产者客户端参数及创建相应的生产者实例(2)构建待发送的消息(3)发送消息(4)关闭生产者实例Producer java api首先,引入 maven 依赖 org.apache.kafka kafka-clients 2.0.0采用默认分区方式将消息散列的发送到各个分区当中import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;public class MyProducer {public static void main(String[ ] args) throws InterruptedException {Properties props &#x3D; new Properties();&#x2F;&#x2F;设置 kafka 集群的地址props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);&#x2F;&#x2F;ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，0不等响应就继续发（可靠性低），1leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）props.put(“acks”, “all”);props.put(“retries”, 3); &#x2F;&#x2F;失败重试次数-&gt;失败会自动重试（可恢复&#x2F;不可恢复）–&gt;(有可能会造成数据的乱序)props.put(“batch.size”, 10); &#x2F;&#x2F;数据发送的批次大小提高效率&#x2F;吞吐量太大会数据延迟props.put(“linger.ms”, 10000); &#x2F;&#x2F;消息在缓冲区保留的时间,超过设置的值就会被提交到服务端props.put(“max.request.size”,10); &#x2F;&#x2F;数据发送请求的最大缓存数props.put(“buffer.memory”, 10240); &#x2F;&#x2F;整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。bootstrap.serverskey.serializervalue.serializer为了防止参数名字符串书写错误,可以使用如下方式进行设置: props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092”);props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());produceDemo代码如下：import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import org.apache.commons.lang3.RandomStringUtils;import java.util.Properties; public class ProducerDemo { private static final String SERVERS&#x3D;”node1:9092,node2:9092,node3:9090”; public static void main(String[] args) { Properties props &#x3D; new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, SERVERS); props.put(ProducerConfig.ACKS_CONFIG, “all”);&#x2F;&#x2F; props.put(ProducerConfig.RETRIES_CONFIG, 3);&#x2F;&#x2F; props.put(ProducerConfig.BATCH_SIZE_CONFIG, 10);&#x2F;&#x2F; props.put(ProducerConfig.LINGER_MS_CONFIG, 10000);&#x2F;&#x2F; props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, 10);&#x2F;&#x2F; props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10240); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, “org.apache.kafka.common.serialization.StringSerializer”); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, “false”); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer(props); for (int i = 0;i&lt;10;i++) &#123; ProducerRecord&lt;String, String&gt; msg = new ProducerRecord&lt;&gt;(&quot;tpc_2&quot;, &quot;name&quot;+i, &quot;0 &quot;+i ); producer.send(msg); &#125; producer.close(); &#125; 2.生产者api参数发送方式（发后即忘）发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题;不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。Future send &#x3D; producer.send(rcd);3.同步发送(sync )try { producer.send(rcd).get();} catch (Exception e) { e.printStackTrace(); }0.8.x 前,有一个参数 producer.type&#x3D;sycn|asycn 来决定生产者的发送模式;现已失效(新版中,producer 在底层只有异步)4.异步发送(async )回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。消费者API1.一个正常的消费逻辑需要具备以下几个步骤:(1)配置消费者客户端参数(2)创建相应的消费者实例;(3)订阅主题;(4)拉取消息并消费;(5)提交消费位移 offset;(6)关闭消费者实例代码如下:import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import java.time.Duration;import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicBoolean;public class ConsumerDemo { private static final String SERVERS &#x3D; “node1:9092,node2:9092,node3:9092”; public static void main(String[] args) throws InterruptedException &#123; //定义一个AtomicBoolean类型的isRunning来控制消费者拉取消息 AtomicBoolean isRunning = new AtomicBoolean(true); //1.参数配置 Properties props = new Properties(); //key的反序列化器 props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); //value的反序列化器 props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName()); //服务器地址 props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,SERVERS); //设置自动读取的起始offset（偏移量），值可以是：earliest，latest，none props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,&quot;earliest&quot;); //设置自动提交offset（偏移量） props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true); //设置消费者组 props.put(ConsumerConfig.GROUP_ID_CONFIG,&quot;b1&quot;); //2.构建consumer实例 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3.订阅主题 consumer.subscribe(Arrays.asList(&quot;tpc_1&quot;)); //4.拉取消息 Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; while (isRunning.get())&#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(Long.MAX_VALUE)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; //do some process做一些处理 System.out.println(record.key()+&quot;,&quot; +record.value()+&quot;,&quot; +record.topic()+&quot;,&quot; +record.partition()+&quot;,&quot; +record.offset()); System.out.println(&quot;------------------------分割线---------------------------&quot;); &#125; &#125; &#125; &#125;); thread.start(); //主线程可以去休眠60s Thread.sleep(60000); //修改isRunning的值为false isRunning.set(false); //5.关闭consumer实例 consumer.close(); &#125; }五、Topic管理API一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法: 1.列出主题ListTopicsResult listTopicsResult &#x3D; adminClient.listTopics();Set topics &#x3D; listTopicsResult.names().get();System.out.println(topics);2.查看主题信息DescribeTopicsResultdescribeTopicsResult&#x3D; &#x3D; adminClient.describeTopics(Arrays.asList(“tpc_3”, “tpc_4”));Map&lt;String, TopicDescription&gt; res &#x3D; describeTopicsResult.all().get();Set ksets &#x3D; res.keySet();for (String k : ksets) { System.out.println(res.get(k));}3.创建主题代码示例:&#x2F;&#x2F; 参数配置Properties props &#x3D; new Properties();props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”);props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000);&#x2F;&#x2F; 创建 admin client 对象AdminClient adminClient &#x3D; KafkaAdminClient.create(props);&#x2F;&#x2F; 由服务端 controller 自行分配分区及副本所在 brokerNewTopic tpc_3 &#x3D; new NewTopic(“tpc_3”, 2, (short) 1);&#x2F;&#x2F; 手动指定分区及副本的 broker 分配HashMap&lt;Integer, List&gt; replicaAssignments &#x3D; new HashMap&lt;&gt;();&#x2F;&#x2F;分区0分配到broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1));&#x2F;&#x2F; 分区 1,分配到 broker0,broker2replicaAssignments.put(0,Arrays.asList(0,1));NewTopic tpc_4 &#x3D; new NewTopic(“tpc_4”, replicaAssignments);CreateTopicsResultresult&#x3D; &#x3D; adminClient.createTopics(Arrays.asList(tpc_3,tpc_4));&#x2F;&#x2F; 从 future 中等待服务端返回try { result.all().get();} catch (Exception e) {e.printStackTrace();}adminClient.close();4.删除主题代码示例:DeleteTopicsResultdeleteTopicsResult&#x3D; &#x3D; adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”));Map&lt;String, KafkaFuture&gt; values &#x3D; deleteTopicsResult.values(); System.out.println(values);5.除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]},{"title":"Kafka命令行操作","slug":"Kafka命令行操作","date":"2022-06-12T09:47:32.000Z","updated":"2022-06-20T22:05:56.396Z","comments":true,"path":"2022/06/12/Kafka命令行操作/","link":"","permalink":"http://example.com/2022/06/12/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"","text":"1.Kafka 中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。 2.创建topic.&#x2F;kafka-topics.sh –zookeeper node1:2181,node2:2181,node3:2181 –create –replication-factor 2 –partitions 2 –topic test手动指定副本的存储位置bin&#x2F;kafka-topics.sh –create –topic tpc_3 –zookeeper node1:2181 –replica-assignment 0:1,1:2该方式下,命令会自动判断所要创建的 topic 的分区数及副本数3.删除topicbin&#x2F;kafka-topics.sh –delete –topic tpc_3 –zookeeper node1：2181删除 topic,需要一个参数处于启用状态: delete.topic.enable &#x3D; true,否则删不掉4.查看topicbin&#x2F;kafka-topics.sh –zookeeper node1:2181,node2:2181,node3:2181 –list5.增加分区数bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper node1:2181Kafka 只支持增加分区,不支持减少分区原因是:减少分区,代价太大(数据的转移,日志段拼接合并) 如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去;6.动态配置topic参数添加、修改配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip删除配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type7.Kafka命令行生产者与消费者操作(1)生产者:kafka-console-producerbin&#x2F;kafka-console-producer.sh –broker-list node1:9092, node2:9092, node3:9092 –topic tpc_1(2)消费者:kafka-console-consumer1消费消息bin&#x2F;kafka-console-consumer.sh –bootstrap-server node1:9092, node2:9092, node1:9092 –topic tpc_1 –from-beginning(从头开始)2指定要消费的分区,和要消费的起始 offsetbin&#x2F;kafka-console-consumer.sh–bootstrap-servernode1:9092,node2:9092,node3:9092 –topic tcp_1 –offset 2 –partition 08.Kafka命令行配置管理比如查看 topic 的配置可以按如下方式执行:bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type topics –entity-name tpc_2比如查看 broker 的动态配置可以按如下方式执行:bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type brokers –entity-name 0 –zookeeper node1:2181","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]},{"title":"Spark-local& stand-alone配置","slug":"Spark local& stand-alone配置","date":"2022-05-22T03:06:37.000Z","updated":"2022-05-23T09:48:11.092Z","comments":true,"path":"2022/05/22/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/22/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一、Spark-local模式本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境 Anaconda On Linux 安装 (单台服务器脚本安装) 1.安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：···#出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27; &gt;&gt;&gt; yes···# 出现添加路径：/export/server/anaconda3···[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3 PREFIX=/export/server/anaconda3··· 2.安装完成后, 退出终端， 重新进来: 1exit 1234结果显示： #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base) [root@master ~]# 3.在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple 4.spark 安装 将文件上传到 &#x2F;export&#x2F;server 里面 ，解压 1234cd /export/server# 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 建立软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 添加环境变量 SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 123456789101112131415161718192021222324252627vim /etc/profile内容：···注：此部分之前配置过，此部分不需要在配置#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar#HADOOP_HOMEexport HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin#ZOOKEEPER_HOMEexport ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin·····#将以下部分添加进去#SPARK_HOME export SPARK_HOME=/export/server/spark#HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop#PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 1234567vim .bashrc内容添加进去：#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241#PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 5.重新加载环境变量文件 12source /etc/profile source ~/.bashrc\\ 进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹 1cd /export/server/anaconda3/envs/pyspark/bin/ 开启 1./pyspark 6.查看WebUI界面 12#浏览器访问：http://master:4040/ 退出 1conda deactivate 二、Spark-Standalone模式Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)1.安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: 1234cd /export/server# 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示： ... # 出现内容选 yes Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27; &gt;&gt;&gt; yes···# 出现添加路径：/export/server/anaconda3···[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3 PREFIX=/export/server/anaconda3··· 2.安装完成后, 退出终端， 重新进来: 1exit 1234结果显示： # 看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base) [root@master ~]# 3.在 node1节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3 1234567#分发 .bashrc : scp ~/.bashrc root@node2:~/ scp ~/.bashrc root@node3:~/#分发 profile : scp /etc/profile/ root@node2:/etc/ scp /etc/profile/ root@node3:/etc/ 4.创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 5.切换到虚拟环境内 12345conda activate pyspark结果显示： (base) [root@master ~]# conda activate pyspark (pyspark) [root@master ~]# 6.在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple 7.master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件 1cd /export/server/spark/conf 8.将文件 workers.template 改名为 workers，并配置文件内容 1234567mv workers.template workers# localhost删除，内容追加文末：node1node2node3# 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker 9.将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 1234567891011121314151617181920212223242526272829303132333435mv spark-env.sh.template spark-env.shvim spark-env.sh文末追加内容：##设置JAVA安装目录 JAVA_HOME=/export/server/jdk## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=master # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=&quot;- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true&quot; 10.开启 hadoop 的 hdfs 和 yarn 集群 123start-dfs.sh start-yarn.sh 11.在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下: 123hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog 12.将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置 1234567891011mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true 13.配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息） 123456789mv log4j.properties.template log4j.properties vim log4j.properties结果显示： ... 18 # Set everything to be logged to the console 19 log4j.rootCategory=WARN, console .... 14.node1节点分发 spark安装文件夹到node2和node3上 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWD scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD 15.node 2和node3上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 16.重新加载环境变量 1234567source /etc/profile进入 /export/server/spark/sbin 文件目录下 启动 start-history-server.shcd /export/server/spark/sbin ./start-history-server.sh 访问WebUI 界面 123浏览器访问： http://master:18080/ 18.启动Spark的Master和Worker进程 1234567891011121314# 启动全部master和worker sbin/start-all.sh # 或者可以一个个启动: # 启动当前机器的master sbin/start-master.sh # 启动当前机器的worker sbin/start-worker.sh# 停止全部 sbin/stop-all.sh # 停止当前机器的master sbin/stop-master.sh # 停止当前机器的worker sbin/stop-worker.sh 访问WebUI界面 123浏览器访问： http://master:8080/","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"Spark HA & Yarn配置","date":"2022-05-22T03:06:37.000Z","updated":"2022-05-24T09:35:11.714Z","comments":true,"path":"2022/05/22/Spark HA & Yarn配置/","link":"","permalink":"http://example.com/2022/05/22/Spark%20HA%20&%20Yarn%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一、Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。 1.此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper2.配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接3.在master节点上重新进行前面配置的 zookeeper 操作 12345678910上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件 在 /export/server 目录下创建软连接 进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 接上步给 zoo.cfg 添加内容 进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 推送成功后，分别在 slave1 和 slave2 上创建软连接 接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处. 4.进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容 123cd /export/server/spark/conf vim spark-env.sh 5.为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 12345结果显示： ...... 82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=master ......... 文末添加内容 123SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER - # spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - # 指定Zookeeper的连接地址Dspark.deploy.zookeeper.dir=/spark-ha&quot; # 指定在Zookeeper中注册临时节点的路径 6.分发 spark-env.sh 到 salve1 和 slave2 上 123scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/ 7.启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: 1234567# 在 master 上 启动一个master 和全部worker /export/server/spark/sbin/start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master /export/server/spark/sbin/start-master.shjps #查看是否启动 8.访问 WebUI 界面 123http://master:8081/http://slave1:8082/ 9.此时 kill 掉 master 上的 master 假设 master 主机宕机掉 12# master主机 master 的进程号 kill -9 41589 10.访问 slave1 的 WebUI 1http://slave1:8082/ 11.进行主备切换的测试 提交一个 spark 任务到当前 活跃的 master上 : 123/export/server/spark/bin/spark-submit --master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 12.复制标签 kill 掉 master 的 进程号 再次访问 master 的 WebUI 123http://master:8081/网页访问不了！ 13.再次访问 slave1 的 WebUI 1http://slave1:8082/ 14.可以看到当前活跃的 master 提示信息 123/export/server/spark/bin/spark-submit --master同样可以输出结果 当新的 master 接收集群后, 程序继续运行, 正常得到结果. 12结论 HA模式下, 主备切换 不会影响到正在运行的程序.最大的影响是 会让它中断大约30秒左右 二、Spark On YARN模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 1.保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） 123456spark-env.sh 文件部分显示：··· 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop···· 2.链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式） 12345bin/pyspark --master yarn --deploy-mode client|cluster# --deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # --deploy-mode 仅可以用在YARN模式下 1bin/spark-shell --master yarn --deploy-mode client|cluster 1bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数 3.park-submit 和 spark-shell 和 pyspark的相关参数 123456- bin/pyspark: pyspark解释器spark环境 - bin/spark-shell: scala解释器spark环境 - bin/spark-submit: 提交jar包或Python文件执行的工具 - bin/spark-sql: sparksql客户端工具这4个客户端工具的参数基本通用.以spark-submit 为例: bin/spark-submit --master spark://master:7077 xxx.py 4.启动 YARN 的历史服务器 123cd /export/server/hadoop-3.3.0/sbin ./mr-jobhistory-daemon.sh start historyserver 5.访问WebUI界面 1http://master:19888/ client 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 1 --total- executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 cluster 模式测试 12345SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode cluster --driver- memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 --conf &quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot; --conf &quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot; $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3","categories":[],"tags":[]},{"title":"spark基础环境配置","slug":"“spark基础环境配置”","date":"2022-05-22T03:06:37.000Z","updated":"2022-05-23T03:02:50.295Z","comments":true,"path":"2022/05/22/“spark基础环境配置”/","link":"","permalink":"http://example.com/2022/05/22/%E2%80%9Cspark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E2%80%9D/","excerpt":"","text":"一、基础环境配置1.编辑主机名（3台机器） 1vim /etc/hostname 2.Hosts映射（3台主机） 1vim /etc/hosts 3.关闭防火墙（3台机器） 12systemctl stop firewalld.service #关闭防火墙systemctl disable firewalld.service #禁止防火墙开启自启 4.设置ssh免密登录（node1执行-&gt;node1|node2|node3） 12ssh-keygen #4个回车生成公钥、私钥ssh-copy-id node1、ssh-copy-id node2、ssh-copy-id node3 5.集群时间同步（3台机器） 12yum-y install ntpdatentpdate ntp4.aliyun.com 6.创建统一工作目录（3台机器） 123mkdir -p /export/server/ #软件安装路径mkdir -p /export/data/ #数据存储路径mkdir -p /export/software/ #安装包存放路径 二、JDK安装1.编译环境软件安装目录 1mkdir-p /export/server/ 2.JDK1.8安装上传jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压 1mkdir-p /export/server/ 3.配置环境变量 1234vim /etc/profile export JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 4.重新加载环境变量文件 1source /etc/profile 5.JDK配置后验证 1java -version 三、Hadoop安装配置1.解压上传文件（ hadoop-3.3.0-Centos7-64-with-snappy.tar.gz ）到&#x2F;export&#x2F;server&#x2F;目录下 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 2.修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#修改hadoop-env.shcd /export/server/hadoop-3.3.0/etc/hadoopvim hadoop-env.sh #进入文件export JAVA_HOME=/export/server/jdk1.8.0_241 #文件最后添加export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root#修改core-site.xml&lt;设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;设置Hadoop本地保存数据路径&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;设置HDFS web UI用户身份&gt;&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;整合hive 用户代理设置&gt;hdfs-site.xmlmarpred-site.xml&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;文件系统垃圾桶保存时间&gt;&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;#修改mapred-site.xml&lt; 设置MR程序默认运行模式： yarn集群模式 local本地模式&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;MR程序历史服务地址&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;MR程序历史服务器web端地址&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;yarn-site.xmlworkers&lt;分发同步hadoop安装包&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;#修改yarn-site.xml&lt; 设置YARN集群主角色运行机器位置 &gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;是否将对容器实施物理内存限制&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;是否将对容器实施虚拟内存限制&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;开启日志聚集&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;设置yarn历史服务器地址&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;历史日志保存的时间 7天&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;#修改workerslocalhostnode1.itcast.cnnode2.itcast.cnnode3.itcast.cn#分发同步hadoop安装包cd /export/serverscp -r hadoop-3.3.0 root@node2:$PWDscp -r hadoop-3.3.0 root@node3:$PWD 3.将hadoop添加到环境变量 123vim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 4.重新加载环境变量文件 1source /etc/profile 5.Hadoop集群启动 格式化namenode（只有首次启动需要格式化） 1hdfs namenode -forma 6.脚本一键启动 1start-all.sh 7.查看WEB页面 12 HDFS集群 :http://node1:9870/YARN集群 :http://node1:8088/ 四、Zookeeper安装配置1.配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 node1.root node2.root node3.root 12345678vim /etc/hosts#结果显示127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.88.135 node1 node1.root192.168.88.136 node2 node2.root192.168.88.137 node3 node3.root 2.zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 12cd /export/server/tar -zxvf zookeeper-3.4.10.tar.gz 3.在 &#x2F;export&#x2F;server 目录下创建软连接 12cd /export/serverln -s zookeeper-3.4.10/ zookeeper 4.cd进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 12cd /export/server/zookeeper/conf/ cp zoo_sample.cfg zoo.cfg 接上步给 zoo.cfg 添加内容 12345678910#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去 123cd /export/server/zookeeper/zkdatatouch myidecho &#x27;1&#x27; &gt; myid 6.将 node1 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给node2 和 node3 12scp -r /export/server/zookeeper-3.4.10/ node2:$PWDscp -r /export/server/zookeeper-3.4.10/ node3:$PWD 7.推送成功后，分别在 node2 和 node3 上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper 接上步推送完成后将 node2 和 node3 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3 8.配置zookeeper的环境变量（注：三台主机都需要配置） 12345vim /etc/profile#zookeeper 环境变量export ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 9.重新加载环境变量文件 1source /etc/profile 10.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F; 目录下启动 zkServer.sh 脚本 （注：三台都需要做） 12cd /export/server/zookeeper/bin/ zkServer.sh start 11.查看zookeeper是否开启 1jps","categories":[],"tags":[]},{"title":"我的第一篇博客文章","slug":"我的第一篇博客文章","date":"2022-05-21T06:57:06.000Z","updated":"2022-05-21T06:57:06.881Z","comments":true,"path":"2022/05/21/我的第一篇博客文章/","link":"","permalink":"http://example.com/2022/05/21/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-05-21T06:51:11.881Z","updated":"2022-05-21T06:51:11.882Z","comments":true,"path":"2022/05/21/hello-world/","link":"","permalink":"http://example.com/2022/05/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]}