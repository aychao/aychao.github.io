{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"Categories","date":"2022-05-21T07:21:50.403Z","updated":"2022-05-21T07:21:50.403Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-21T07:21:50.412Z","updated":"2022-05-21T07:21:50.412Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"About","date":"2022-05-21T07:21:50.399Z","updated":"2022-05-21T07:21:50.399Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"spark基础环境配置","slug":"“spark基础环境配置”","date":"2022-05-22T03:06:37.000Z","updated":"2022-05-23T03:02:50.295Z","comments":true,"path":"2022/05/22/“spark基础环境配置”/","link":"","permalink":"http://example.com/2022/05/22/%E2%80%9Cspark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E2%80%9D/","excerpt":"","text":"一、基础环境配置1.编辑主机名（3台机器） 1vim /etc/hostname 2.Hosts映射（3台主机） 1vim /etc/hosts 3.关闭防火墙（3台机器） 12systemctl stop firewalld.service #关闭防火墙systemctl disable firewalld.service #禁止防火墙开启自启 4.设置ssh免密登录（node1执行-&gt;node1|node2|node3） 12ssh-keygen #4个回车生成公钥、私钥ssh-copy-id node1、ssh-copy-id node2、ssh-copy-id node3 5.集群时间同步（3台机器） 12yum-y install ntpdatentpdate ntp4.aliyun.com 6.创建统一工作目录（3台机器） 123mkdir -p /export/server/ #软件安装路径mkdir -p /export/data/ #数据存储路径mkdir -p /export/software/ #安装包存放路径 二、JDK安装1.编译环境软件安装目录 1mkdir-p /export/server/ 2.JDK1.8安装上传jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压 1mkdir-p /export/server/ 3.配置环境变量 1234vim /etc/profile export JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 4.重新加载环境变量文件 1source /etc/profile 5.JDK配置后验证 1java -version 三、Hadoop安装配置1.解压上传文件（ hadoop-3.3.0-Centos7-64-with-snappy.tar.gz ）到&#x2F;export&#x2F;server&#x2F;目录下 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 2.修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#修改hadoop-env.shcd /export/server/hadoop-3.3.0/etc/hadoopvim hadoop-env.sh #进入文件export JAVA_HOME=/export/server/jdk1.8.0_241 #文件最后添加export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root#修改core-site.xml&lt;设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;设置Hadoop本地保存数据路径&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;设置HDFS web UI用户身份&gt;&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;整合hive 用户代理设置&gt;hdfs-site.xmlmarpred-site.xml&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;文件系统垃圾桶保存时间&gt;&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;#修改mapred-site.xml&lt; 设置MR程序默认运行模式： yarn集群模式 local本地模式&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;MR程序历史服务地址&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;MR程序历史服务器web端地址&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;yarn-site.xmlworkers&lt;分发同步hadoop安装包&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;#修改yarn-site.xml&lt; 设置YARN集群主角色运行机器位置 &gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;是否将对容器实施物理内存限制&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;是否将对容器实施虚拟内存限制&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;开启日志聚集&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;设置yarn历史服务器地址&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;历史日志保存的时间 7天&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;#修改workerslocalhostnode1.itcast.cnnode2.itcast.cnnode3.itcast.cn#分发同步hadoop安装包cd /export/serverscp -r hadoop-3.3.0 root@node2:$PWDscp -r hadoop-3.3.0 root@node3:$PWD 3.将hadoop添加到环境变量 123vim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 4.重新加载环境变量文件 1source /etc/profile 5.Hadoop集群启动 格式化namenode（只有首次启动需要格式化） 1hdfs namenode -forma 6.脚本一键启动 1start-all.sh 7.查看WEB页面 12 HDFS集群 :http://node1:9870/YARN集群 :http://node1:8088/ 四、Zookeeper安装配置1.配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 node1.root node2.root node3.root 12345678vim /etc/hosts#结果显示127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.88.135 node1 node1.root192.168.88.136 node2 node2.root192.168.88.137 node3 node3.root 2.zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 12cd /export/server/tar -zxvf zookeeper-3.4.10.tar.gz 3.在 &#x2F;export&#x2F;server 目录下创建软连接 12cd /export/serverln -s zookeeper-3.4.10/ zookeeper 4.cd进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 12cd /export/server/zookeeper/conf/ cp zoo_sample.cfg zoo.cfg 接上步给 zoo.cfg 添加内容 12345678910#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去 123cd /export/server/zookeeper/zkdatatouch myidecho &#x27;1&#x27; &gt; myid 6.将 node1 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给node2 和 node3 12scp -r /export/server/zookeeper-3.4.10/ node2:$PWDscp -r /export/server/zookeeper-3.4.10/ node3:$PWD 7.推送成功后，分别在 node2 和 node3 上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper 接上步推送完成后将 node2 和 node3 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3 8.配置zookeeper的环境变量（注：三台主机都需要配置） 12345vim /etc/profile#zookeeper 环境变量export ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 9.重新加载环境变量文件 1source /etc/profile 10.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F; 目录下启动 zkServer.sh 脚本 （注：三台都需要做） 12cd /export/server/zookeeper/bin/ zkServer.sh start 11.查看zookeeper是否开启 1jps","categories":[],"tags":[]},{"title":"我的第一篇博客文章","slug":"我的第一篇博客文章","date":"2022-05-21T06:57:06.000Z","updated":"2022-05-21T06:57:06.881Z","comments":true,"path":"2022/05/21/我的第一篇博客文章/","link":"","permalink":"http://example.com/2022/05/21/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-05-21T06:51:11.881Z","updated":"2022-05-21T06:51:11.882Z","comments":true,"path":"2022/05/21/hello-world/","link":"","permalink":"http://example.com/2022/05/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}